using CSV
using DataFrames
using Dates
using TimeZones
using Statistics
using Plots
using YAML

# ============================================================
# Uncertainty Engine (minimal skeleton)
#
# Goal (v0):
# - load Yahoo Finance hourly CSV (as produced by download_data.py)
# - define perturbations:
#   (1) volatility scaling (scale log-returns)
#   (2) window shifts (re-anchor path starting later)
# - produce an overlay plot: original vs perturbed paths
# ============================================================

# -----------------------------
# CONFIG (edit these)
# -----------------------------


const INPUT_CSV  = "data/historical_prices.csv"
const ASSUMPTIONS_YAML = "assumptions/assumptions.yaml"
const PRICE_COL  = :Close               # default; can be overridden by assumptions.yaml
const OUTPUT_PNG = "plots/perturbation_overlays.png"
const OUTPUT_REPORT = "reports/decision_validity_report.txt"   # default; can be overridden by assumptions.yaml
const MIN_ROWS_DEFAULT = 200      # minimum number of price bars required (heuristic)
const MIN_DAYS_DEFAULT = 20       # minimum number of distinct trading days required (heuristic)
# Perturbations
const VOL_SCALES    = [0.5, 1.0, 1.5, 2.0]   # scale factor for σ (via log-return scaling)
const WINDOW_SHIFTS = [0, 3, 6, 12]          # shift in rows (hours) for "start later" overlays

# Finance-specific uncertainty knobs
const EXECUTION_SPREAD_BPS = 10.0          # default; can be overridden by assumptions.yaml (execution.assumed_spread_bps)
const BENCHMARK_TICKER = "SPY"             # default; can be overridden by assumptions.yaml (benchmark.ticker)
const LOW_LIQUIDITY_Q = 0.10               # default; can be overridden by assumptions.yaml (liquidity.low_liquidity_quantile)

# -----------------------------
# IO: Load Yahoo Finance CSV
# -----------------------------

"""
load_prices(path) -> DataFrame

Reads the Yahoo Finance CSV generated by our Python downloader.

That CSV has 3 header rows:
1) Price, Adj Close, Close, ...
2) Ticker, AAPL, AAPL, ...
3) Datetime, , , ...
Then data starts at row 4.

We read from row 4 onward and assign our own column names.
"""
function load_prices(path::AbstractString)::DataFrame
    df = CSV.File(
        path;
        header=false,
        skipto=4,                    # data begins at row 4
        normalizenames=false,
        missingstring=["", "NA", "NaN"]
    ) |> DataFrame

    rename!(df, [:Datetime, :AdjClose, :Close, :High, :Low, :Open, :Volume])

    # Datetime comes in like "2025-12-22 14:30:00+00:00" (string).
    # Parse robustly to ZonedDateTime in UTC.
    df.Datetime = map(df.Datetime) do s
        if s isa ZonedDateTime
            return s
        end
        str = String(s)

        # Yahoo's CSV uses a space between date and time, e.g.:
        # "2025-12-22 14:30:00+00:00"
        # TimeZones' ISO parser expects a 'T' delimiter, so we try both.
        try
            return ZonedDateTime(str)
        catch
            return ZonedDateTime(replace(str, " " => "T"))
        end
    end

    sort!(df, :Datetime)
    return df
end

# -----------------------------
# One-sentence decision verdict
# -----------------------------

"""
decision_verdict(summary) -> String

Creates a single human-readable verdict by ranking uncertainty channels
from the printed stability summary.

Heuristics (v0):
- Window-length dominance if volatility varies strongly across window lengths.
- Overnight dominance if overnight gap std is comparable to or larger than intraday vol.
- Outlier dominance if removing 1 jump reduces volatility a lot.
- Liquidity/source/market-factor only if the corresponding blocks are available.

Returns a one-sentence statement like:
"Decision unstable: dominated by window-length choice and overnight gaps."
"""
function decision_verdict(
    summary::Dict;
    n_rows::Int,
    n_days::Int,
    min_rows::Int=MIN_ROWS_DEFAULT,
    min_days::Int=MIN_DAYS_DEFAULT
)::Dict

    need_rows = max(0, min_rows - n_rows)
    need_days = max(0, min_days - n_days)

    if need_rows > 0 || need_days > 0
        extra = String[]
        need_rows > 0 && push!(extra, "$(need_rows) more bars (rows)")
        need_days > 0 && push!(extra, "$(need_days) more trading days")
        extra_str = join(extra, " and ")

        sentence = "Decision verdict unavailable: insufficient historical data (have $(n_rows) bars across $(n_days) days; require at least $(min_rows) bars and $(min_days) days)."
        recommendation = "Collect $(extra_str) before making any buy/sell decision based on this uncertainty analysis."

        return Dict(
            :severity => :unavailable,
            :dominant_drivers => String[],
            :invalidity_probability => 1.0,
            :sentence => sentence,
            :recommendation => recommendation,
            :data_needed => Dict(
                :need_rows => need_rows,
                :need_days => need_days,
                :min_rows => min_rows,
                :min_days => min_days,
                :n_rows => n_rows,
                :n_days => n_days
            )
        )
    end

    base = get(summary, :base_volatility, NaN)
    if !(base isa Real) || isnan(base) || base <= 0
        # Return a structured verdict with minimal info
        return Dict(
            :severity => :unavailable,
            :dominant_drivers => String[],
            :invalidity_probability => 1.0,
            :sentence => "Decision verdict unavailable: insufficient data for base volatility.",
            :recommendation => "You should with 100% probability NOT base a buy/sell decision solely on this historical data."
        )
    end

    drivers = Vector{Pair{Symbol, Float64}}()

    # 1) window length sensitivity: use relative spread between max and min window vol
    win = get(summary, :window_length_volatility, Dict())
    if win isa Dict && !isempty(win)
        vals = Float64[float(v) for v in values(win) if v isa Real && !isnan(float(v))]
        if !isempty(vals)
            spread = (maximum(vals) - minimum(vals)) / base
            push!(drivers, :window_length => spread)
        end
    end

    # 2) overnight gap dominance: compare overnight gap std to intraday vol
    og = get(summary, :overnight_gap_std, NaN)
    if og isa Real && !isnan(float(og))
        ratio = float(og) / base
        push!(drivers, :overnight_gaps => ratio)
    end

    # 3) outlier dominance: how much does volatility drop if we remove 1 jump
    out = get(summary, :outlier_removed_volatility, NaN)
    if out isa Real && !isnan(float(out))
        drop = (base - float(out)) / base
        push!(drivers, :single_jump_outlier => drop)
    end

        # serial dependence / effective sample size collapse
    neff = get(summary, :effective_sample_size, nothing)
    if neff isa Integer
        n = max(1, n_rows - 1)
        frac_lost = 1.0 - (float(neff) / float(n))
        push!(drivers, :serial_dependence => frac_lost)
    end

    # regime change proxy (CUSUM)
    cus = get(summary, :cusum_score, NaN)
    cthr = get(summary, :cusum_threshold, NaN)
    if cus isa Real && cthr isa Real && !isnan(float(cus)) && !isnan(float(cthr)) && float(cthr) > 0
        push!(drivers, :regime_change => float(cus) / float(cthr))
    end

    # 4) liquidity regime dominance (if available)
    liq = get(summary, :liquidity, Dict())
    if liq isa Dict && get(liq, :available, false) == true
        r = get(liq, :vol_ratio_lowliq_over_all, NaN)
        if r isa Real && !isnan(float(r))
            push!(drivers, :low_liquidity_regime => float(r) - 1.0)
        end
    end

    # 5) price source sensitivity (if available)
    src = get(summary, :price_source, Dict())
    if src isa Dict && get(src, :available, false) == true
        r = get(src, :vol_ratio_adj_over_close, NaN)
        if r isa Real && !isnan(float(r))
            push!(drivers, :price_definition => abs(float(r) - 1.0))
        end
    end

    # 6) market factor dominance (if available)
    mkt = get(summary, :market_factor, Dict())
    if mkt isa Dict && get(mkt, :available, false) == true
        r2 = get(mkt, :r2, NaN)
        if r2 isa Real && !isnan(float(r2))
            push!(drivers, :market_factor => float(r2))
        end
    end

    if isempty(drivers)
        return Dict(
            :severity => :stable,
            :dominant_drivers => String[],
            :invalidity_probability => 0.20,
            :sentence => "Decision stable (no dominant uncertainty channel detected in this run).",
            :recommendation => "You should with 20% probability NOT base a buy/sell decision solely on this historical data."
        )
    end

    # Rank drivers (largest first)
    sort!(drivers, by = x -> x.second, rev=true)
    top = drivers[1:min(2, length(drivers))]

    # Severity classification from the strongest driver
    strongest = top[1].second
    severity = :stable
    if strongest >= 1.0
        severity = :unstable
    elseif strongest >= 0.5
        severity = :fragile
    elseif strongest >= 0.25
        severity = :conditional_stability
    end

    sev_str = (severity == :conditional_stability) ? "conditionally stable" : String(severity)

    # Pretty driver names
    name(d::Symbol) = d == :window_length      ? "window-length choice" :
                      d == :overnight_gaps     ? "overnight gaps" :
                      d == :single_jump_outlier ? "single-jump outliers" :
                      d == :low_liquidity_regime ? "low-liquidity regime" :
                      d == :price_definition   ? "price-definition (Close vs AdjClose)" :
                      d == :market_factor      ? "market-factor dominance" :
                      d == :serial_dependence ? "serial dependence (effective sample size)" :
                      d == :regime_change     ? "regime change (CUSUM)" :
                                                  String(d)

    drivers_str = join((name(p.first) for p in top), " and ")

    # Map severity to a decision invalidity probability (heuristic, conservative)
    invalidity_prob =
        severity == :unstable               ? 0.85 :
        severity == :fragile                ? 0.65 :
        severity == :conditional_stability  ? 0.45 :
                                              0.20

    sentence = "Decision $(sev_str): dominated by $(drivers_str)."
    recommendation = "You should with $(round(invalidity_prob * 100))% probability NOT base a buy/sell decision solely on this historical data."

    return Dict(
        :severity => severity,
        :dominant_drivers => [p.first for p in top],
        :invalidity_probability => invalidity_prob,
        :sentence => sentence,
        :recommendation => recommendation
    )
end

# -----------------------------
# Action semantics + placeholder decision mapping
# -----------------------------

"""
Placeholder mapping from data -> BUY/SELL/HOLD semantics.
NOT a recommendation — it defines what “act” means.

Momentum signal:
m = log(p_end / p_end-H)

Act if |m| > threshold_sigma * sigma_base * sqrt(H).
"""
function action_from_data(p::Vector{Float64}, summary::Dict;
                          horizon_hours::Int=24, threshold_sigma::Real=0.5)

    n = length(p)
    n < 5 && return (:HOLD, NaN, NaN)

    H = clamp(horizon_hours, 1, n-1)
    m = log(p[end] / p[end-H])

    σ = get(summary, :base_volatility, NaN)
    if !(σ isa Real) || isnan(float(σ)) || σ <= 0
        return (:HOLD, m, NaN)
    end

    thr = float(threshold_sigma) * float(σ) * sqrt(H)
    if m > thr
        return (:BUY, m, thr)
    elseif m < -thr
        return (:SELL, m, thr)
    else
        return (:HOLD, m, thr)
    end
end
# -----------------------------
# Report writer
# -----------------------------

"""
write_report(path; assumptions_applied, summary, verdict, plot_path)

Writes a plain-text report so results are reproducible and easy to share.
"""
function write_report(
    path::AbstractString;
    assumptions_applied::Dict,
    summary::Dict,
    verdict,
    plot_path::AbstractString
)
    mkpath(dirname(path))
    open(path, "w") do io
        println(io, "DECISION VALIDITY REPORT")
        println(io, "========================")
        println(io)
        println(io, "Generated (UTC): ", Dates.format(now(Dates.UTC), dateformat"yyyy-mm-dd HH:MM:SS"))
        println(io)
        println(io, "SUMMARY STATEMENT")
        println(io, "-----------------")
        println(io, verdict[:sentence])
        println(io, verdict[:recommendation])
        println(io)
        println(io, "INTERPRETATION")
        println(io, "--------------")
        println(io, "This assessment evaluates how sensitive a trading decision is to")
        println(io, "reasonable assumption changes (time window choice, volatility regime,")
        println(io, "overnight gaps, execution friction, and data conventions).")
        println(io)
        println(io, "A high invalidity probability means that small, plausible changes in")
        println(io, "assumptions lead to materially different outcomes.")
        println(io)
        println(io, "DOMINANT UNCERTAINTY SOURCES")
        println(io, "----------------------------")
        for d in verdict[:dominant_drivers]
            println(io, " - ", d)
        end
        println(io)
        println(io, "NUMERICAL SUMMARY (more readable)")
        println(io, "--------------------------------")

        # Helpers: format returns-volatility-like numbers as percentages per bar
        fmt_pct(x) = string(round(100 * float(x); digits=2), "%")
        fmt_num(x) = string(round(float(x); digits=4))
        function fmt_vol(x)
            (x isa Real && !isnan(float(x))) ? fmt_pct(x) : "n/a"
        end

        base = get(summary, :base_volatility, NaN)
        madv = get(summary, :base_volatility_mad, NaN)
        og   = get(summary, :overnight_gap_std, NaN)
        boot_ci = get(summary, :bootstrap_vol_ci, (NaN, NaN))
        boot_w  = get(summary, :bootstrap_vol_ci_width, NaN)
        neff = get(summary, :effective_sample_size, missing)
        cus  = get(summary, :cusum_score, NaN)
        cthr = get(summary, :cusum_threshold, NaN)

        println(io, " - Base volatility (std, per bar):              ", fmt_vol(base))
        println(io, " - Robust volatility (MAD→sigma, per bar):      ", fmt_vol(madv))
        if (base isa Real && !isnan(float(base)) && base > 0) && (madv isa Real && !isnan(float(madv)))
            println(io, " - Robust/Std ratio:                            ", fmt_num(madv / base))
        end

        println(io, " - Overnight gap std (per day boundary):         ", fmt_vol(og))
        if (base isa Real && !isnan(float(base)) && base > 0) && (og isa Real && !isnan(float(og)))
            println(io, " - Overnight/Base ratio:                         ", fmt_num(float(og) / float(base)))
        end

        # Window-length sensitivity (prints as percentages)
        win = get(summary, :window_length_volatility, Dict())
        if win isa Dict && !isempty(win)
            parts = String[]
            for (L, v) in sort!(collect(win); by=x->x[1])
                push!(parts, string(L, "h: ", fmt_vol(v)))
            end
            println(io, " - Volatility by window length:                  ", join(parts, ", "))
        else
            println(io, " - Volatility by window length:                  n/a")
        end

        outv = get(summary, :outlier_removed_volatility, NaN)
        println(io, " - Volatility after removing top jump:           ", fmt_vol(outv))
        if (base isa Real && !isnan(float(base)) && base > 0) && (outv isa Real && !isnan(float(outv)))
            println(io, " - Outlier dominance (drop/base):                ", fmt_num((float(base) - float(outv)) / float(base)))
        end

        # Bootstrap CI
        if boot_ci isa Tuple && length(boot_ci) == 2
            lo, hi = boot_ci
            println(io, " - Bootstrap vol CI (per bar):                   [", fmt_vol(lo), ", ", fmt_vol(hi), "]")
        else
            println(io, " - Bootstrap vol CI (per bar):                   n/a")
        end
        println(io, " - Bootstrap CI width (per bar):                 ", fmt_vol(boot_w))

        # Effective sample size
        if neff === missing
            println(io, " - Effective sample size (n_eff):                n/a")
        else
            println(io, " - Effective sample size (n_eff):                ", neff)
        end

        # CUSUM regime proxy
        if cus isa Real && cthr isa Real && !isnan(float(cus)) && !isnan(float(cthr)) && float(cthr) > 0
            println(io, " - CUSUM score / threshold:                      ", fmt_num(float(cus) / float(cthr)),
                    "  (raw=", fmt_num(cus), ", thr=", fmt_num(cthr), ")")
        else
            println(io, " - CUSUM score / threshold:                      n/a")
        end

        # Availability blocks (kept brief)
        src = get(summary, :price_source, Dict())
        liq = get(summary, :liquidity, Dict())
        mkt = get(summary, :market_factor, Dict())
        println(io, " - Price source check available:                 ", (src isa Dict && get(src, :available, false)) ? "yes" : "no")
        println(io, " - Liquidity check available:                    ", (liq isa Dict && get(liq, :available, false)) ? "yes" : "no")
        println(io, " - Market factor check available:                ", (mkt isa Dict && get(mkt, :available, false)) ? "yes" : "no")
        println(io)
        println(io, "ASSUMPTIONS APPLIED")
        println(io, "-------------------")
        for (k, v) in assumptions_applied
            println(io, " - ", k, " = ", v)
        end
        println(io)
        println(io, "NOTE")
        println(io, "----")
        println(io, "This report does NOT predict prices.")
        println(io, "It evaluates whether a decision based on this data is defensible.")
    end
end

# -----------------------------
# Assumptions (YAML) loader
# -----------------------------

"""
load_assumptions(path) -> Dict

Loads assumptions.yaml as a nested Dict. If the file is missing or invalid,
returns an empty Dict so the engine can still run with defaults.
"""
function load_assumptions(path::AbstractString)::Dict
    try
        y = YAML.load_file(path)
        return y isa Dict ? y : Dict()
    catch err
        @warn "Could not load assumptions.yaml, using defaults" path err
        return Dict()
    end
end

"""
get_nested(d, keys, default)

Safely fetch nested keys from a Dict, e.g. get_nested(a, ["execution","assumed_spread_bps"], 10.0)
"""
function get_nested(d::Dict, keys::Vector{String}, default)
    cur = d
    for k in keys
        if cur isa Dict && haskey(cur, k)
            cur = cur[k]
        else
            return default
        end
    end
    return cur
end

# Convert YAML strings like "Close" -> Symbol(:Close)
to_symbol(x, default::Symbol) = x === nothing ? default : Symbol(String(x))

# -----------------------------
# Helpers: series extraction
# -----------------------------

"""
extract_series(df; price_col=:Close)

Returns:
- t::Vector{ZonedDateTime}
- p::Vector{Float64}
"""
function extract_series(df::DataFrame; price_col::Symbol=PRICE_COL)
    t = Vector{ZonedDateTime}(df.Datetime)
    p = Float64.(df[!, price_col])
    return t, p
end

"""
log_returns(p)

rᵢ = log(pᵢ / pᵢ₋₁)
length(r) = length(p)-1
"""
log_returns(p::AbstractVector{<:Real}) = log.(p[2:end] ./ p[1:end-1])

"""
reconstruct_from_log_returns(p0, r)

p[1]=p0; p[i]=p0*exp(sum(r[1:i-1]))
"""
function reconstruct_from_log_returns(p0::Real, r::AbstractVector{<:Real})
    out = Vector{Float64}(undef, length(r) + 1)
    out[1] = float(p0)
    acc = 0.0
    @inbounds for i in 2:length(out)
        acc += float(r[i-1])
        out[i] = out[1] * exp(acc)
    end
    return out
end

# -----------------------------
# Uncertainty tools
# -----------------------------

"Robust scale estimate via MAD (median absolute deviation)."
function mad_sigma(x::AbstractVector{<:Real}; c::Real=1.4826)
    m = median(x)
    mad = median(abs.(x .- m))
    return float(c) * float(mad)
end

"""
Effective sample size for correlated time series.

n_eff ≈ n / (1 + 2 * Σ ρ_k), k=1..K
Correlation reduces independent information (physics/stat).
"""
function effective_sample_size(r::AbstractVector{<:Real}; maxlag::Int=12)
    n = length(r)
    n < 5 && return n
    μ = mean(r)
    x = float.(r .- μ)
    v = sum(abs2, x) / n
    v == 0 && return n

    s = 0.0
    K = min(maxlag, n - 2)
    for k in 1:K
        num = sum(x[1:end-k] .* x[1+k:end]) / (n - k)
        ρ = num / v
        ρ = clamp(ρ, -0.5, 0.95)
        s += ρ
    end
    denom = 1 + 2s
    denom <= 0 && return n
    return max(1, Int(round(n / denom)))
end

"Block bootstrap for volatility (preserves short-range dependence)."
function block_bootstrap_vol(r::AbstractVector{<:Real}; B::Int=500, blocklen::Int=6, estimator::Symbol=:std, mad_c::Real=1.4826)
    n = length(r)
    n < 5 && return Float64[]
    bl = clamp(blocklen, 1, n)
    nblocks = Int(ceil(n / bl))

    out = Vector{Float64}(undef, B)
    rF = float.(r)

    for b in 1:B
        idxs = Int[]
        for _ in 1:nblocks
            start = rand(1:(n - bl + 1))
            append!(idxs, start:(start + bl - 1))
        end
        idxs = idxs[1:n]
        sample = rF[idxs]
        out[b] = estimator == :mad ? mad_sigma(sample; c=mad_c) : std(sample)
    end
    return out
end

"CUSUM-like regime change score (proxy, v0)."
function cusum_score(r::AbstractVector{<:Real}; sigma::Real)
    n = length(r)
    n < 5 && return 0.0
    σ = float(sigma)
    σ <= 0 && return 0.0
    z = float.(r) ./ σ
    cs = cumsum(z)
    return maximum(cs) - minimum(cs)
end

# -----------------------------
# Perturbation 1: volatility scaling
# -----------------------------

"""
perturb_volatility(p; scale)

Scales log-returns by `scale`:
r' = scale * r

This increases/decreases volatility while keeping the same "shape" of return signs.
Returns perturbed price path of same length as p.
"""
function perturb_volatility(p::Vector{Float64}; scale::Real)
    r = log_returns(p)
    r2 = float(scale) .* r
    return reconstruct_from_log_returns(p[1], r2)
end

# -----------------------------
# Perturbation 2: window shift overlays
# -----------------------------

"""
perturb_window_shift(t, p; shift)

Shift the path by `shift` rows (hours) and re-anchor to the same initial price.

Interpretation:
"what if you had started observing / deciding `shift` hours later?"

Returns:
- t2: same time vector (for x-axis alignment)
- p2: shifted/re-anchored price vector with NaN where undefined
"""
function perturb_window_shift(t::Vector{ZonedDateTime}, p::Vector{Float64}; shift::Int)
    n = length(p)
    if shift <= 0
        return t, copy(p)
    end
    if shift >= n - 1
        return t, fill(NaN, n)
    end

    p_sub = p[(1 + shift):end]

    # Re-anchor to match initial price
    r_sub = log_returns(p_sub)
    p_re  = reconstruct_from_log_returns(p[1], r_sub)

    p2 = fill(NaN, n)
    p2[(1 + shift):end] .= p_re
    return t, p2
end

# -----------------------------
# Perturbation 2b: execution friction (bid-ask / slippage proxy)
# -----------------------------

"""
perturb_execution_friction(p; spread_bps=10.0, trades=1)

A crude transaction-cost proxy:
- Assume each trade costs `spread_bps` basis points (bps) in price impact.
- Convert bps -> multiplicative cost factor: (1 - spread_bps/1e4) per trade.
- Apply the cost `trades` times to the final portfolio value (worst-case).

This is not a microstructure model; it's a "decision robustness" stressor:
if your edge disappears under tiny friction, the decision is unstable.
"""
function perturb_execution_friction(p::Vector{Float64}; spread_bps::Real=EXECUTION_SPREAD_BPS, trades::Int=1)
    cost = (1.0 - float(spread_bps) / 1e4) ^ max(trades, 0)
    p2 = copy(p)
    p2[end] *= cost
    return p2
end

# -----------------------------
# Perturbation 2c: price source / corporate-action sensitivity
# -----------------------------

"""
price_source_sensitivity(df)

Compares Close vs AdjClose return statistics.
Why it matters:
- AdjClose includes split/dividend adjustments (corporate actions).
- If your conclusions change strongly between Close and AdjClose,
  your decision depends on accounting conventions, not signal.

Returns a Dict with vol/drift deltas when both series exist.
"""
function price_source_sensitivity(df::DataFrame)
    has_close = :Close in names(df)
    has_adj   = :AdjClose in names(df)
    if !(has_close && has_adj)
        return Dict(:available => false)
    end

    pC = Float64.(df.Close)
    pA = Float64.(df.AdjClose)

    rC = log_returns(pC)
    rA = log_returns(pA)

    return Dict(
        :available => true,
        :vol_close => std(rC),
        :vol_adjclose => std(rA),
        :drift_close => mean(rC),
        :drift_adjclose => mean(rA),
        :vol_ratio_adj_over_close => std(rA) / std(rC),
        :drift_diff_adj_minus_close => mean(rA) - mean(rC),
    )
end

# -----------------------------
# Perturbation 2d: liquidity / volume regime sensitivity
# -----------------------------

"""
liquidity_sensitivity(df; q=0.10)

Uses Volume as a proxy for liquidity.
- Compute volatility on ALL returns
- Compute volatility only on "low liquidity" rows (bottom q quantile of volume)
If low-liquidity volatility is much higher, your risk is dominated by thin trading.

Returns a Dict with vol_all, vol_lowliq, and the fraction of low-liquidity points.
"""
function liquidity_sensitivity(df::DataFrame; q::Real=LOW_LIQUIDITY_Q)
    if !(:Volume in names(df)) || !(:Close in names(df))
        return Dict(:available => false)
    end
    vol = Float64.(df.Volume)
    p   = Float64.(df.Close)

    r = log_returns(p)
    # returns r correspond to transitions (i -> i+1). Use volume at i+1 as proxy for that bar.
    vol_r = vol[2:end]

    thresh = quantile(vol_r, float(q))
    mask = vol_r .<= thresh

    return Dict(
        :available => true,
        :q => float(q),
        :vol_all => std(r),
        :vol_lowliq => std(r[mask]),
        :lowliq_fraction => mean(mask),
        :vol_ratio_lowliq_over_all => std(r[mask]) / std(r),
    )
end

# -----------------------------
# Perturbation 2e: market-factor dominance (beta / R²)
# -----------------------------

"""
market_factor_dominance(df; benchmark="SPY")

If the CSV contains BOTH the asset and a benchmark series, estimate:
r_asset ≈ α + β r_bench

Outputs beta and R² (fraction of variance explained by the market factor).
Why it matters:
- If R² is high, your "signal" is mostly market exposure, not stock-specific.
- If decisions depend on stock-specific behavior but R²≈1, you're fooling yourself.

This function is 'best-effort':
- If benchmark isn't present, returns Dict(:available=>false).
"""
function market_factor_dominance(df::DataFrame; benchmark::AbstractString=BENCHMARK_TICKER)
    # Support two simple layouts:
    # (1) columns: Close and Close_<BENCH> (user-prepared merge)
    # (2) columns: BenchClose (rare). We implement (1) robustly.
    bench_sym = Symbol("Close_" * benchmark)

    if !(:Close in names(df)) || !(bench_sym in names(df))
        return Dict(:available => false, :needed_column => String(bench_sym))
    end

    pA = Float64.(df.Close)
    pB = Float64.(df[!, bench_sym])

    rA = log_returns(pA)
    rB = log_returns(pB)

    n = min(length(rA), length(rB))
    rA = rA[end-n+1:end]
    rB = rB[end-n+1:end]

    # OLS beta = cov(rA,rB)/var(rB)
    β = cov(rA, rB) / var(rB)
    α = mean(rA) - β * mean(rB)

    # R²
    yhat = α .+ β .* rB
    ss_res = sum((rA .- yhat) .^ 2)
    ss_tot = sum((rA .- mean(rA)) .^ 2)
    R2 = ss_tot == 0 ? NaN : (1 - ss_res / ss_tot)

    return Dict(
        :available => true,
        :benchmark => benchmark,
        :beta => β,
        :alpha => α,
        :r2 => R2
    )
end

# -----------------------------
# Perturbation 3: window-length sensitivity
# -----------------------------

"""
window_length_sensitivity(p; lengths)

Evaluate sensitivity of statistics to different window lengths.
Returns a Dict: window_length => volatility_estimate
"""
function window_length_sensitivity(p::Vector{Float64}; lengths::Vector{Int})
    out = Dict{Int, Float64}()
    for L in lengths
        if L < 3 || L > length(p)
            continue
        end
        r = log_returns(p[end-L+1:end])
        out[L] = std(r)
    end
    return out
end

# -----------------------------
# Perturbation 4: outlier / jump sensitivity
# -----------------------------

"""
outlier_sensitivity(p; k=1)

Removes the k largest absolute log-returns and recomputes volatility.
Returns the modified volatility.
"""
function outlier_sensitivity(p::Vector{Float64}; k::Int=1)
    r = log_returns(p)
    idx = sortperm(abs.(r), rev=true)
    keep = setdiff(1:length(r), idx[1:min(k, length(idx))])
    r2 = r[keep]
    return std(r2)
end

# -----------------------------
# Perturbation 5: session-boundary / overnight gap
# -----------------------------

"""
overnight_gap_series(t, p)

Computes log-gap between last close of day and first open of next day.
"""
function overnight_gap_series(t::Vector{ZonedDateTime}, p::Vector{Float64})
    days = Date.(t)
    gaps = Float64[]
    for d in unique(days)[2:end]
        prev_idx = findlast(days .== d - Day(1))
        curr_idx = findfirst(days .== d)
        if prev_idx !== nothing && curr_idx !== nothing
            push!(gaps, log(p[curr_idx] / p[prev_idx]))
        end
    end
    return gaps
end

# -----------------------------
# Plotting: overlay plot
# -----------------------------

function overlay_plot(
    t::Vector{ZonedDateTime},
    p::Vector{Float64};
    vol_scales=VOL_SCALES,
    shifts=WINDOW_SHIFTS,
    spread_bps::Real=EXECUTION_SPREAD_BPS,
    outpath::AbstractString=OUTPUT_PNG
)
    # Convert ZonedDateTime -> DateTime for Plots (keeps UTC)
    x = DateTime.(t)

    plt = plot(
        x, p;
        label="Observed",
        linewidth=4,
        title="Decision robustness stress test",
        xlabel="time (UTC)",
        ylabel="price",
        legend=:topright,
        legendfontsize=9,
    )

    # Collect all perturbation paths so we can build BOTH aggregate "uncertainty" envelopes.
    # We compute envelopes across perturbations:
    # - Worst = pointwise MIN (conservative)
    # - Best  = pointwise MAX (optimistic)
    pert_paths = Vector{Vector{Float64}}()

    # Volatility scaling overlays
    for s in vol_scales
        if s == 1.0
            continue
        end
        p2 = perturb_volatility(p; scale=s)
        plot!(plt, x, p2; label="Vol ×$(s)", linewidth=1)
        push!(pert_paths, p2)
    end

    # Execution friction overlay (single stress scenario)
    p_fric = perturb_execution_friction(p; spread_bps=spread_bps, trades=1)
    plot!(plt, x, p_fric; label="Cost $(spread_bps)bps", linewidth=1)
    push!(pert_paths, p_fric)

    # Window shift overlays
    for sh in shifts
        if sh == 0
            continue
        end
        _, p2 = perturb_window_shift(t, p; shift=sh)
        plot!(plt, x, p2; label="Shift +$(sh)h", linewidth=1)
        push!(pert_paths, p2)
    end

    if !isempty(pert_paths)
        # pointwise envelopes ignoring NaNs (window shifts introduce NaNs at the beginning)
        worst = similar(p)
        best  = similar(p)

        for i in eachindex(p)
            vals = Float64[]
            for pp in pert_paths
                v = pp[i]
                if !isnan(v)
                    push!(vals, v)
                end
            end
            if isempty(vals)
                worst[i] = NaN
                best[i]  = NaN
            else
                worst[i] = minimum(vals)   # conservative envelope
                best[i]  = maximum(vals)   # optimistic envelope
            end
        end

        plot!(plt, x, worst; label="Worst", linewidth=5, color=:red)
        plot!(plt, x, best;  label="Best",  linewidth=5, color=:green)
    end

    mkpath(dirname(outpath))
    savefig(plt, outpath)
    return plt
end

# -----------------------------
# Stability / dominance summary
# -----------------------------

"""
stability_summary(
    df::DataFrame,
    p::Vector{Float64},
    t::Vector{ZonedDateTime};
    window_lengths::Vector{Int}=[6, 12, 24],
    low_liq_q::Real=LOW_LIQUIDITY_Q,
    benchmark_ticker::AbstractString=BENCHMARK_TICKER,
    benchmark_enabled::Bool=false
)

Produces a simple dominance report over uncertainty channels.
"""
function stability_summary(
    df::DataFrame,
    p::Vector{Float64},
    t::Vector{ZonedDateTime};
    window_lengths::Vector{Int}=[6, 12, 24],
    low_liq_q::Real=LOW_LIQUIDITY_Q,
    benchmark_ticker::AbstractString=BENCHMARK_TICKER,
    benchmark_enabled::Bool=false,
    autocorr_max_lag::Int=12,
    robust_mad_c::Real=1.4826,
    bootstrap_enabled::Bool=true,
    bootstrap_runs::Int=500,
    blocklen::Int=6,
    ci_lo::Real=0.05,
    ci_hi::Real=0.95,
    cusum_enabled::Bool=true,
    cusum_threshold::Real=8.0
)
    base_vol = std(log_returns(p))

    r = log_returns(p)
    base_mad = mad_sigma(r; c=robust_mad_c)
    n_eff = effective_sample_size(r; maxlag=autocorr_max_lag)

    win = window_length_sensitivity(p; lengths=window_lengths)
    out = outlier_sensitivity(p; k=1)
    gaps = overnight_gap_series(t, p)

    boot_ci = (NaN, NaN)
    boot_width = NaN
    if bootstrap_enabled
        boot = block_bootstrap_vol(r; B=bootstrap_runs, blocklen=blocklen, estimator=:std, mad_c=robust_mad_c)
        if !isempty(boot)
            qs = quantile(boot, [float(ci_lo), float(ci_hi)])
            boot_ci = (qs[1], qs[2])
            boot_width = qs[2] - qs[1]
        end
    end

    cus = NaN
    if cusum_enabled
        cus = cusum_score(r; sigma=(base_mad > 0 ? base_mad : base_vol))
    end

    src = price_source_sensitivity(df)
    liq = liquidity_sensitivity(df; q=low_liq_q)
    mkt = benchmark_enabled ? market_factor_dominance(df; benchmark=benchmark_ticker) :
                              Dict(:available => false, :benchmark_enabled => false)

    return Dict(
        :base_volatility => base_vol,
        :window_length_volatility => win,
        :outlier_removed_volatility => out,
        :overnight_gap_std => isempty(gaps) ? NaN : std(gaps),
        :price_source => src,
        :liquidity => liq,
        :market_factor => mkt,
        :base_volatility_mad => base_mad,
        :effective_sample_size => n_eff,
        :bootstrap_vol_ci => boot_ci,
        :bootstrap_vol_ci_width => boot_width,
        :cusum_score => cus,
        :cusum_threshold => cusum_threshold,
    )
end

# -----------------------------
# Main
# -----------------------------

if abspath(PROGRAM_FILE) == @__FILE__
    assumptions = load_assumptions(ASSUMPTIONS_YAML)

    input_csv = String(get_nested(assumptions, ["data","path"], INPUT_CSV))
    out_png   = String(get_nested(assumptions, ["reporting","overlay_plot_path"], OUTPUT_PNG))
    out_report = String(get_nested(assumptions, ["reporting","report_path"], OUTPUT_REPORT))

    price_primary = to_symbol(get_nested(assumptions, ["price_definition","primary"], String(PRICE_COL)), PRICE_COL)
    spread_bps    = Float64(get_nested(assumptions, ["execution","assumed_spread_bps"], EXECUTION_SPREAD_BPS))
    low_liq_q     = Float64(get_nested(assumptions, ["liquidity","low_liquidity_quantile"], LOW_LIQUIDITY_Q))

    win_lengths_any = get_nested(assumptions, ["volatility","window_lengths_hours"], [6, 12, 24])
    window_lengths = Int.(win_lengths_any)

    bench_enabled = Bool(get_nested(assumptions, ["benchmark","enabled"], false))
    bench_ticker  = String(get_nested(assumptions, ["benchmark","ticker"], BENCHMARK_TICKER))


        min_rows = Int(get_nested(assumptions, ["decision","min_rows"], MIN_ROWS_DEFAULT))
    min_days = Int(get_nested(assumptions, ["decision","min_trading_days"], MIN_DAYS_DEFAULT))

    autocorr_max_lag = Int(get_nested(assumptions, ["returns","autocorr_max_lag"], 12))
    mad_c = Float64(get_nested(assumptions, ["volatility","mad_to_sigma"], 1.4826))

    boot_enabled = Bool(get_nested(assumptions, ["risk_model","bootstrap_enabled"], true))
    boot_runs = Int(get_nested(assumptions, ["risk_model","bootstrap_runs"], 500))
    boot_blocklen = Int(get_nested(assumptions, ["risk_model","block_bootstrap_blocklen"], 6))
    ci = get_nested(assumptions, ["risk_model","bootstrap_ci"], [0.05, 0.95])
    ci_lo = Float64(ci[1])
    ci_hi = Float64(ci[2])

    cusum_enabled = Bool(get_nested(assumptions, ["market_structure","cusum_enabled"], true))
    cusum_threshold = Float64(get_nested(assumptions, ["market_structure","cusum_threshold"], 8.0))

    action_buy = String(get_nested(assumptions, ["decision","action_semantics","buy"], "enter or increase a long position"))
    action_sell = String(get_nested(assumptions, ["decision","action_semantics","sell"], "reduce or exit a long position"))
    action_hold = String(get_nested(assumptions, ["decision","action_semantics","hold"], "do nothing / wait for more data"))

    rule_kind = String(get_nested(assumptions, ["decision","rule","kind"], "momentum"))
    rule_horizon = Int(get_nested(assumptions, ["decision","rule","horizon_hours"], 24))
    rule_thr_sigma = Float64(get_nested(assumptions, ["decision","rule","threshold_sigma"], 0.5))

    df = load_prices(input_csv)
    t, p = extract_series(df; price_col=price_primary)

    n_rows = nrow(df)
    n_days = length(unique(Date.(t)))

    println("Loaded $(length(p)) rows")
    println("Time range: ", first(t), " → ", last(t))
    println("Note: with interval=\"1h\", Yahoo only returns MARKET HOURS, not 24/7. So 7 days ≠ 7*24 rows.")

    println("Assumptions applied:")
    println("  input_csv       = ", input_csv)
    println("  price_primary   = ", price_primary)
    println("  spread_bps      = ", spread_bps)
    println("  low_liq_q       = ", low_liq_q)
    println("  window_lengths  = ", window_lengths)
    println("  benchmark       = ", bench_enabled ? bench_ticker : "(disabled)")
    println("  overlay_out_png = ", out_png)
    assumptions_applied = Dict(
        "input_csv" => input_csv,
        "price_primary" => string(price_primary),
        "spread_bps" => spread_bps,
        "low_liq_q" => low_liq_q,
        "window_lengths" => window_lengths,
        "benchmark" => (bench_enabled ? bench_ticker : "(disabled)"),
        "overlay_out_png" => out_png,
        "report_path" => out_report,
    )

    overlay_plot(t, p; spread_bps=spread_bps, outpath=out_png)
summary = stability_summary(
    df, p, t;
    window_lengths=window_lengths,
    low_liq_q=low_liq_q,
    benchmark_ticker=bench_ticker,
    benchmark_enabled=bench_enabled,
    autocorr_max_lag=autocorr_max_lag,
    robust_mad_c=mad_c,
    bootstrap_enabled=boot_enabled,
    bootstrap_runs=boot_runs,
    blocklen=boot_blocklen,
    ci_lo=ci_lo,
    ci_hi=ci_hi,
    cusum_enabled=cusum_enabled,
    cusum_threshold=cusum_threshold
)
    println("Stability / uncertainty summary:")
    for (k, v) in summary
        println("  ", k, " => ", v)
    end
    verdict = decision_verdict(summary; n_rows=n_rows, n_days=n_days, min_rows=min_rows, min_days=min_days)
    println(verdict[:sentence])
    println(verdict[:recommendation])
    println("Action semantics: BUY=$(action_buy); SELL=$(action_sell); HOLD=$(action_hold)")

    # IMPORTANT: we never suggest acting unless the decision is stable enough.
    if rule_kind == "momentum" && (verdict[:severity] == :stable || verdict[:severity] == :conditional_stability)
        act, m, thr = action_from_data(p, summary; horizon_hours=rule_horizon, threshold_sigma=rule_thr_sigma)
        println("Placeholder action (", rule_kind, "): ", act, "  (momentum=", m, ", threshold=", thr, ")")
        println("Note: this defines what 'act' would mean IF the decision is stable; it is NOT financial advice.")
    else
        println("Placeholder action: HOLD (decision not stable enough, insufficient data, or rule disabled)")
    end
    write_report(out_report; assumptions_applied=assumptions_applied, summary=summary, verdict=verdict, plot_path=out_png)
    println("Saved report to $(out_report)")
    println("Saved overlay plot to $(out_png)")
end