using CSV
using DataFrames
using Dates
using TimeZones
using Statistics
using Plots
using YAML

# ============================================================
# Uncertainty Engine (minimal skeleton)
#
# Goal (v0):
# - load Yahoo Finance hourly CSV (as produced by download_data.py)
# - define perturbations:
#   (1) volatility scaling (scale log-returns)
#   (2) window shifts (re-anchor path starting later)
# - produce an overlay plot: original vs perturbed paths
# ============================================================

# -----------------------------
# CONFIG (edit these)
# -----------------------------


const INPUT_CSV  = "data/historical_prices.csv"
const ASSUMPTIONS_YAML = "assumptions/assumptions.yaml"
const PRICE_COL  = :Close               # default; can be overridden by assumptions.yaml
const OUTPUT_PNG = "plots/perturbation_overlays.png"
const OUTPUT_REPORT = "reports/decision_validity_report.txt"   # default; can be overridden by assumptions.yaml

# Perturbations
const VOL_SCALES    = [0.5, 1.0, 1.5, 2.0]   # scale factor for σ (via log-return scaling)
const WINDOW_SHIFTS = [0, 3, 6, 12]          # shift in rows (hours) for "start later" overlays

# Finance-specific uncertainty knobs
const EXECUTION_SPREAD_BPS = 10.0          # default; can be overridden by assumptions.yaml (execution.assumed_spread_bps)
const BENCHMARK_TICKER = "SPY"             # default; can be overridden by assumptions.yaml (benchmark.ticker)
const LOW_LIQUIDITY_Q = 0.10               # default; can be overridden by assumptions.yaml (liquidity.low_liquidity_quantile)

# -----------------------------
# IO: Load Yahoo Finance CSV
# -----------------------------

"""
load_prices(path) -> DataFrame

Reads the Yahoo Finance CSV generated by our Python downloader.

That CSV has 3 header rows:
1) Price, Adj Close, Close, ...
2) Ticker, AAPL, AAPL, ...
3) Datetime, , , ...
Then data starts at row 4.

We read from row 4 onward and assign our own column names.
"""
function load_prices(path::AbstractString)::DataFrame
    df = CSV.File(
        path;
        header=false,
        skipto=4,                    # data begins at row 4
        normalizenames=false,
        missingstring=["", "NA", "NaN"]
    ) |> DataFrame

    rename!(df, [:Datetime, :AdjClose, :Close, :High, :Low, :Open, :Volume])

    # Datetime comes in like "2025-12-22 14:30:00+00:00" (string).
    # Parse robustly to ZonedDateTime in UTC.
    df.Datetime = map(df.Datetime) do s
        if s isa ZonedDateTime
            return s
        end
        str = String(s)

        # Yahoo's CSV uses a space between date and time, e.g.:
        # "2025-12-22 14:30:00+00:00"
        # TimeZones' ISO parser expects a 'T' delimiter, so we try both.
        try
            return ZonedDateTime(str)
        catch
            return ZonedDateTime(replace(str, " " => "T"))
        end
    end

    sort!(df, :Datetime)
    return df
end

# -----------------------------
# One-sentence decision verdict
# -----------------------------

"""
decision_verdict(summary) -> String

Creates a single human-readable verdict by ranking uncertainty channels
from the printed stability summary.

Heuristics (v0):
- Window-length dominance if volatility varies strongly across window lengths.
- Overnight dominance if overnight gap std is comparable to or larger than intraday vol.
- Outlier dominance if removing 1 jump reduces volatility a lot.
- Liquidity/source/market-factor only if the corresponding blocks are available.

Returns a one-sentence statement like:
"Decision unstable: dominated by window-length choice and overnight gaps."
"""
function decision_verdict(summary::Dict)::Dict
    base = get(summary, :base_volatility, NaN)
    if !(base isa Real) || isnan(base) || base <= 0
        # Return a structured verdict with minimal info
        return Dict(
            :severity => :unavailable,
            :dominant_drivers => String[],
            :invalidity_probability => 1.0,
            :sentence => "Decision verdict unavailable: insufficient data for base volatility.",
            :recommendation => "You should with 100% probability NOT base a buy/sell decision solely on this historical data."
        )
    end

    drivers = Vector{Pair{Symbol, Float64}}()

    # 1) window length sensitivity: use relative spread between max and min window vol
    win = get(summary, :window_length_volatility, Dict())
    if win isa Dict && !isempty(win)
        vals = Float64[float(v) for v in values(win) if v isa Real && !isnan(float(v))]
        if !isempty(vals)
            spread = (maximum(vals) - minimum(vals)) / base
            push!(drivers, :window_length => spread)
        end
    end

    # 2) overnight gap dominance: compare overnight gap std to intraday vol
    og = get(summary, :overnight_gap_std, NaN)
    if og isa Real && !isnan(float(og))
        ratio = float(og) / base
        push!(drivers, :overnight_gaps => ratio)
    end

    # 3) outlier dominance: how much does volatility drop if we remove 1 jump
    out = get(summary, :outlier_removed_volatility, NaN)
    if out isa Real && !isnan(float(out))
        drop = (base - float(out)) / base
        push!(drivers, :single_jump_outlier => drop)
    end

    # 4) liquidity regime dominance (if available)
    liq = get(summary, :liquidity, Dict())
    if liq isa Dict && get(liq, :available, false) == true
        r = get(liq, :vol_ratio_lowliq_over_all, NaN)
        if r isa Real && !isnan(float(r))
            push!(drivers, :low_liquidity_regime => float(r) - 1.0)
        end
    end

    # 5) price source sensitivity (if available)
    src = get(summary, :price_source, Dict())
    if src isa Dict && get(src, :available, false) == true
        r = get(src, :vol_ratio_adj_over_close, NaN)
        if r isa Real && !isnan(float(r))
            push!(drivers, :price_definition => abs(float(r) - 1.0))
        end
    end

    # 6) market factor dominance (if available)
    mkt = get(summary, :market_factor, Dict())
    if mkt isa Dict && get(mkt, :available, false) == true
        r2 = get(mkt, :r2, NaN)
        if r2 isa Real && !isnan(float(r2))
            push!(drivers, :market_factor => float(r2))
        end
    end

    if isempty(drivers)
        return Dict(
            :severity => :stable,
            :dominant_drivers => String[],
            :invalidity_probability => 0.20,
            :sentence => "Decision stable (no dominant uncertainty channel detected in this run).",
            :recommendation => "You should with 20% probability NOT base a buy/sell decision solely on this historical data."
        )
    end

    # Rank drivers (largest first)
    sort!(drivers, by = x -> x.second, rev=true)
    top = drivers[1:min(2, length(drivers))]

    # Severity classification from the strongest driver
    strongest = top[1].second
    severity = :stable
    if strongest >= 1.0
        severity = :unstable
    elseif strongest >= 0.5
        severity = :fragile
    elseif strongest >= 0.25
        severity = :conditional_stability
    end

    sev_str = (severity == :conditional_stability) ? "conditionally stable" : String(severity)

    # Pretty driver names
    name(d::Symbol) = d == :window_length      ? "window-length choice" :
                      d == :overnight_gaps     ? "overnight gaps" :
                      d == :single_jump_outlier ? "single-jump outliers" :
                      d == :low_liquidity_regime ? "low-liquidity regime" :
                      d == :price_definition   ? "price-definition (Close vs AdjClose)" :
                      d == :market_factor      ? "market-factor dominance" :
                                                  String(d)

    drivers_str = join((name(p.first) for p in top), " and ")

    # Map severity to a decision invalidity probability (heuristic, conservative)
    invalidity_prob =
        severity == :unstable               ? 0.85 :
        severity == :fragile                ? 0.65 :
        severity == :conditional_stability  ? 0.45 :
                                              0.20

    sentence = "Decision $(sev_str): dominated by $(drivers_str)."
    recommendation = "You should with $(round(invalidity_prob * 100))% probability NOT base a buy/sell decision solely on this historical data."

    return Dict(
        :severity => severity,
        :dominant_drivers => [p.first for p in top],
        :invalidity_probability => invalidity_prob,
        :sentence => sentence,
        :recommendation => recommendation
    )
end

# -----------------------------
# Report writer
# -----------------------------

"""
write_report(path; assumptions_applied, summary, verdict, plot_path)

Writes a plain-text report so results are reproducible and easy to share.
"""
function write_report(
    path::AbstractString;
    assumptions_applied::Dict,
    summary::Dict,
    verdict,
    plot_path::AbstractString
)
    mkpath(dirname(path))
    open(path, "w") do io
        println(io, "DECISION VALIDITY REPORT")
        println(io, "========================")
        println(io)
        println(io, "Generated (UTC): ", Dates.format(now(Dates.UTC), dateformat"yyyy-mm-dd HH:MM:SS"))
        println(io)
        println(io, "SUMMARY STATEMENT")
        println(io, "-----------------")
        println(io, verdict[:sentence])
        println(io, verdict[:recommendation])
        println(io)
        println(io, "INTERPRETATION")
        println(io, "--------------")
        println(io, "This assessment evaluates how sensitive a trading decision is to")
        println(io, "reasonable assumption changes (time window choice, volatility regime,")
        println(io, "overnight gaps, execution friction, and data conventions).")
        println(io)
        println(io, "A high invalidity probability means that small, plausible changes in")
        println(io, "assumptions lead to materially different outcomes.")
        println(io)
        println(io, "DOMINANT UNCERTAINTY SOURCES")
        println(io, "----------------------------")
        for d in verdict[:dominant_drivers]
            println(io, " - ", d)
        end
        println(io)
        println(io, "NUMERICAL SUMMARY")
        println(io, "-----------------")
        for (k, v) in summary
            println(io, " - ", k, ": ", v)
        end
        println(io)
        println(io, "ASSUMPTIONS APPLIED")
        println(io, "-------------------")
        for (k, v) in assumptions_applied
            println(io, " - ", k, " = ", v)
        end
        println(io)
        println(io, "NOTE")
        println(io, "----")
        println(io, "This report does NOT predict prices.")
        println(io, "It evaluates whether a decision based on this data is defensible.")
    end
end

# -----------------------------
# Assumptions (YAML) loader
# -----------------------------

"""
load_assumptions(path) -> Dict

Loads assumptions.yaml as a nested Dict. If the file is missing or invalid,
returns an empty Dict so the engine can still run with defaults.
"""
function load_assumptions(path::AbstractString)::Dict
    try
        y = YAML.load_file(path)
        return y isa Dict ? y : Dict()
    catch err
        @warn "Could not load assumptions.yaml, using defaults" path err
        return Dict()
    end
end

"""
get_nested(d, keys, default)

Safely fetch nested keys from a Dict, e.g. get_nested(a, ["execution","assumed_spread_bps"], 10.0)
"""
function get_nested(d::Dict, keys::Vector{String}, default)
    cur = d
    for k in keys
        if cur isa Dict && haskey(cur, k)
            cur = cur[k]
        else
            return default
        end
    end
    return cur
end

# Convert YAML strings like "Close" -> Symbol(:Close)
to_symbol(x, default::Symbol) = x === nothing ? default : Symbol(String(x))

# -----------------------------
# Helpers: series extraction
# -----------------------------

"""
extract_series(df; price_col=:Close)

Returns:
- t::Vector{ZonedDateTime}
- p::Vector{Float64}
"""
function extract_series(df::DataFrame; price_col::Symbol=PRICE_COL)
    t = Vector{ZonedDateTime}(df.Datetime)
    p = Float64.(df[!, price_col])
    return t, p
end

"""
log_returns(p)

rᵢ = log(pᵢ / pᵢ₋₁)
length(r) = length(p)-1
"""
log_returns(p::AbstractVector{<:Real}) = log.(p[2:end] ./ p[1:end-1])

"""
reconstruct_from_log_returns(p0, r)

p[1]=p0; p[i]=p0*exp(sum(r[1:i-1]))
"""
function reconstruct_from_log_returns(p0::Real, r::AbstractVector{<:Real})
    out = Vector{Float64}(undef, length(r) + 1)
    out[1] = float(p0)
    acc = 0.0
    @inbounds for i in 2:length(out)
        acc += float(r[i-1])
        out[i] = out[1] * exp(acc)
    end
    return out
end

# -----------------------------
# Perturbation 1: volatility scaling
# -----------------------------

"""
perturb_volatility(p; scale)

Scales log-returns by `scale`:
r' = scale * r

This increases/decreases volatility while keeping the same "shape" of return signs.
Returns perturbed price path of same length as p.
"""
function perturb_volatility(p::Vector{Float64}; scale::Real)
    r = log_returns(p)
    r2 = float(scale) .* r
    return reconstruct_from_log_returns(p[1], r2)
end

# -----------------------------
# Perturbation 2: window shift overlays
# -----------------------------

"""
perturb_window_shift(t, p; shift)

Shift the path by `shift` rows (hours) and re-anchor to the same initial price.

Interpretation:
"what if you had started observing / deciding `shift` hours later?"

Returns:
- t2: same time vector (for x-axis alignment)
- p2: shifted/re-anchored price vector with NaN where undefined
"""
function perturb_window_shift(t::Vector{ZonedDateTime}, p::Vector{Float64}; shift::Int)
    n = length(p)
    if shift <= 0
        return t, copy(p)
    end
    if shift >= n - 1
        return t, fill(NaN, n)
    end

    p_sub = p[(1 + shift):end]

    # Re-anchor to match initial price
    r_sub = log_returns(p_sub)
    p_re  = reconstruct_from_log_returns(p[1], r_sub)

    p2 = fill(NaN, n)
    p2[(1 + shift):end] .= p_re
    return t, p2
end

# -----------------------------
# Perturbation 2b: execution friction (bid-ask / slippage proxy)
# -----------------------------

"""
perturb_execution_friction(p; spread_bps=10.0, trades=1)

A crude transaction-cost proxy:
- Assume each trade costs `spread_bps` basis points (bps) in price impact.
- Convert bps -> multiplicative cost factor: (1 - spread_bps/1e4) per trade.
- Apply the cost `trades` times to the final portfolio value (worst-case).

This is not a microstructure model; it's a "decision robustness" stressor:
if your edge disappears under tiny friction, the decision is unstable.
"""
function perturb_execution_friction(p::Vector{Float64}; spread_bps::Real=EXECUTION_SPREAD_BPS, trades::Int=1)
    cost = (1.0 - float(spread_bps) / 1e4) ^ max(trades, 0)
    p2 = copy(p)
    p2[end] *= cost
    return p2
end

# -----------------------------
# Perturbation 2c: price source / corporate-action sensitivity
# -----------------------------

"""
price_source_sensitivity(df)

Compares Close vs AdjClose return statistics.
Why it matters:
- AdjClose includes split/dividend adjustments (corporate actions).
- If your conclusions change strongly between Close and AdjClose,
  your decision depends on accounting conventions, not signal.

Returns a Dict with vol/drift deltas when both series exist.
"""
function price_source_sensitivity(df::DataFrame)
    has_close = :Close in names(df)
    has_adj   = :AdjClose in names(df)
    if !(has_close && has_adj)
        return Dict(:available => false)
    end

    pC = Float64.(df.Close)
    pA = Float64.(df.AdjClose)

    rC = log_returns(pC)
    rA = log_returns(pA)

    return Dict(
        :available => true,
        :vol_close => std(rC),
        :vol_adjclose => std(rA),
        :drift_close => mean(rC),
        :drift_adjclose => mean(rA),
        :vol_ratio_adj_over_close => std(rA) / std(rC),
        :drift_diff_adj_minus_close => mean(rA) - mean(rC),
    )
end

# -----------------------------
# Perturbation 2d: liquidity / volume regime sensitivity
# -----------------------------

"""
liquidity_sensitivity(df; q=0.10)

Uses Volume as a proxy for liquidity.
- Compute volatility on ALL returns
- Compute volatility only on "low liquidity" rows (bottom q quantile of volume)
If low-liquidity volatility is much higher, your risk is dominated by thin trading.

Returns a Dict with vol_all, vol_lowliq, and the fraction of low-liquidity points.
"""
function liquidity_sensitivity(df::DataFrame; q::Real=LOW_LIQUIDITY_Q)
    if !(:Volume in names(df)) || !(:Close in names(df))
        return Dict(:available => false)
    end
    vol = Float64.(df.Volume)
    p   = Float64.(df.Close)

    r = log_returns(p)
    # returns r correspond to transitions (i -> i+1). Use volume at i+1 as proxy for that bar.
    vol_r = vol[2:end]

    thresh = quantile(vol_r, float(q))
    mask = vol_r .<= thresh

    return Dict(
        :available => true,
        :q => float(q),
        :vol_all => std(r),
        :vol_lowliq => std(r[mask]),
        :lowliq_fraction => mean(mask),
        :vol_ratio_lowliq_over_all => std(r[mask]) / std(r),
    )
end

# -----------------------------
# Perturbation 2e: market-factor dominance (beta / R²)
# -----------------------------

"""
market_factor_dominance(df; benchmark="SPY")

If the CSV contains BOTH the asset and a benchmark series, estimate:
r_asset ≈ α + β r_bench

Outputs beta and R² (fraction of variance explained by the market factor).
Why it matters:
- If R² is high, your "signal" is mostly market exposure, not stock-specific.
- If decisions depend on stock-specific behavior but R²≈1, you're fooling yourself.

This function is 'best-effort':
- If benchmark isn't present, returns Dict(:available=>false).
"""
function market_factor_dominance(df::DataFrame; benchmark::AbstractString=BENCHMARK_TICKER)
    # Support two simple layouts:
    # (1) columns: Close and Close_<BENCH> (user-prepared merge)
    # (2) columns: BenchClose (rare). We implement (1) robustly.
    bench_sym = Symbol("Close_" * benchmark)

    if !(:Close in names(df)) || !(bench_sym in names(df))
        return Dict(:available => false, :needed_column => String(bench_sym))
    end

    pA = Float64.(df.Close)
    pB = Float64.(df[!, bench_sym])

    rA = log_returns(pA)
    rB = log_returns(pB)

    n = min(length(rA), length(rB))
    rA = rA[end-n+1:end]
    rB = rB[end-n+1:end]

    # OLS beta = cov(rA,rB)/var(rB)
    β = cov(rA, rB) / var(rB)
    α = mean(rA) - β * mean(rB)

    # R²
    yhat = α .+ β .* rB
    ss_res = sum((rA .- yhat) .^ 2)
    ss_tot = sum((rA .- mean(rA)) .^ 2)
    R2 = ss_tot == 0 ? NaN : (1 - ss_res / ss_tot)

    return Dict(
        :available => true,
        :benchmark => benchmark,
        :beta => β,
        :alpha => α,
        :r2 => R2
    )
end

# -----------------------------
# Perturbation 3: window-length sensitivity
# -----------------------------

"""
window_length_sensitivity(p; lengths)

Evaluate sensitivity of statistics to different window lengths.
Returns a Dict: window_length => volatility_estimate
"""
function window_length_sensitivity(p::Vector{Float64}; lengths::Vector{Int})
    out = Dict{Int, Float64}()
    for L in lengths
        if L < 3 || L > length(p)
            continue
        end
        r = log_returns(p[end-L+1:end])
        out[L] = std(r)
    end
    return out
end

# -----------------------------
# Perturbation 4: outlier / jump sensitivity
# -----------------------------

"""
outlier_sensitivity(p; k=1)

Removes the k largest absolute log-returns and recomputes volatility.
Returns the modified volatility.
"""
function outlier_sensitivity(p::Vector{Float64}; k::Int=1)
    r = log_returns(p)
    idx = sortperm(abs.(r), rev=true)
    keep = setdiff(1:length(r), idx[1:min(k, length(idx))])
    r2 = r[keep]
    return std(r2)
end

# -----------------------------
# Perturbation 5: session-boundary / overnight gap
# -----------------------------

"""
overnight_gap_series(t, p)

Computes log-gap between last close of day and first open of next day.
"""
function overnight_gap_series(t::Vector{ZonedDateTime}, p::Vector{Float64})
    days = Date.(t)
    gaps = Float64[]
    for d in unique(days)[2:end]
        prev_idx = findlast(days .== d - Day(1))
        curr_idx = findfirst(days .== d)
        if prev_idx !== nothing && curr_idx !== nothing
            push!(gaps, log(p[curr_idx] / p[prev_idx]))
        end
    end
    return gaps
end

# -----------------------------
# Plotting: overlay plot
# -----------------------------

function overlay_plot(
    t::Vector{ZonedDateTime},
    p::Vector{Float64};
    vol_scales=VOL_SCALES,
    shifts=WINDOW_SHIFTS,
    spread_bps::Real=EXECUTION_SPREAD_BPS,
    outpath::AbstractString=OUTPUT_PNG
)
    # Convert ZonedDateTime -> DateTime for Plots (keeps UTC)
    x = DateTime.(t)

    plt = plot(
        x, p;
        label="original",
        linewidth=4,
        title="Perturbation overlays",
        xlabel="time (UTC)",
        ylabel=String(PRICE_COL),
    )

    # Collect all perturbation paths so we can build ONE aggregate "uncertainty" line.
    # We define the aggregate as the pointwise MIN across all perturbations (conservative / worst-case price path).
    # If you prefer the upper envelope, replace `minimum` with `maximum` below.
    pert_paths = Vector{Vector{Float64}}()

    # Volatility scaling overlays
    for s in vol_scales
        if s == 1.0
            continue
        end
        p2 = perturb_volatility(p; scale=s)
        plot!(plt, x, p2; label="vol_scale=$(s)", linewidth=1)
        push!(pert_paths, p2)
    end

    # Execution friction overlay (single stress scenario)
    p_fric = perturb_execution_friction(p; spread_bps=spread_bps, trades=1)
    plot!(plt, x, p_fric; label="friction=$(spread_bps)bps", linewidth=1)
    push!(pert_paths, p_fric)

    # Window shift overlays
    for sh in shifts
        if sh == 0
            continue
        end
        _, p2 = perturb_window_shift(t, p; shift=sh)
        plot!(plt, x, p2; label="shift=$(sh)h", linewidth=1)
        push!(pert_paths, p2)
    end

    if !isempty(pert_paths)
        # pointwise min ignoring NaNs (window shifts introduce NaNs at the beginning)
        agg = similar(p)
        for i in eachindex(p)
            vals = Float64[]
            for pp in pert_paths
                v = pp[i]
                if !isnan(v)
                    push!(vals, v)
                end
            end
            agg[i] = isempty(vals) ? NaN : minimum(vals)
        end

        plot!(plt, x, agg; label="ALL uncertainties (min envelope)", linewidth=5, color=:red)
    end

    mkpath(dirname(outpath))
    savefig(plt, outpath)
    return plt
end

# -----------------------------
# Stability / dominance summary
# -----------------------------

"""
stability_summary(
    df::DataFrame,
    p::Vector{Float64},
    t::Vector{ZonedDateTime};
    window_lengths::Vector{Int}=[6, 12, 24],
    low_liq_q::Real=LOW_LIQUIDITY_Q,
    benchmark_ticker::AbstractString=BENCHMARK_TICKER,
    benchmark_enabled::Bool=false
)

Produces a simple dominance report over uncertainty channels.
"""
function stability_summary(
    df::DataFrame,
    p::Vector{Float64},
    t::Vector{ZonedDateTime};
    window_lengths::Vector{Int}=[6, 12, 24],
    low_liq_q::Real=LOW_LIQUIDITY_Q,
    benchmark_ticker::AbstractString=BENCHMARK_TICKER,
    benchmark_enabled::Bool=false
)
    base_vol = std(log_returns(p))

    win = window_length_sensitivity(p; lengths=window_lengths)
    out = outlier_sensitivity(p; k=1)
    gaps = overnight_gap_series(t, p)

    src = price_source_sensitivity(df)
    liq = liquidity_sensitivity(df; q=low_liq_q)
    mkt = benchmark_enabled ? market_factor_dominance(df; benchmark=benchmark_ticker) :
                              Dict(:available => false, :benchmark_enabled => false)

    return Dict(
        :base_volatility => base_vol,
        :window_length_volatility => win,
        :outlier_removed_volatility => out,
        :overnight_gap_std => isempty(gaps) ? NaN : std(gaps),
        :price_source => src,
        :liquidity => liq,
        :market_factor => mkt,
    )
end

# -----------------------------
# Main
# -----------------------------

if abspath(PROGRAM_FILE) == @__FILE__
    assumptions = load_assumptions(ASSUMPTIONS_YAML)

    input_csv = String(get_nested(assumptions, ["data","path"], INPUT_CSV))
    out_png   = String(get_nested(assumptions, ["reporting","overlay_plot_path"], OUTPUT_PNG))
    out_report = String(get_nested(assumptions, ["reporting","report_path"], OUTPUT_REPORT))

    price_primary = to_symbol(get_nested(assumptions, ["price_definition","primary"], String(PRICE_COL)), PRICE_COL)
    spread_bps    = Float64(get_nested(assumptions, ["execution","assumed_spread_bps"], EXECUTION_SPREAD_BPS))
    low_liq_q     = Float64(get_nested(assumptions, ["liquidity","low_liquidity_quantile"], LOW_LIQUIDITY_Q))

    win_lengths_any = get_nested(assumptions, ["volatility","window_lengths_hours"], [6, 12, 24])
    window_lengths = Int.(win_lengths_any)

    bench_enabled = Bool(get_nested(assumptions, ["benchmark","enabled"], false))
    bench_ticker  = String(get_nested(assumptions, ["benchmark","ticker"], BENCHMARK_TICKER))

    df = load_prices(input_csv)
    t, p = extract_series(df; price_col=price_primary)

    println("Loaded $(length(p)) rows")
    println("Time range: ", first(t), " → ", last(t))
    println("Note: with interval=\"1h\", Yahoo only returns MARKET HOURS, not 24/7. So 7 days ≠ 7*24 rows.")

    println("Assumptions applied:")
    println("  input_csv       = ", input_csv)
    println("  price_primary   = ", price_primary)
    println("  spread_bps      = ", spread_bps)
    println("  low_liq_q       = ", low_liq_q)
    println("  window_lengths  = ", window_lengths)
    println("  benchmark       = ", bench_enabled ? bench_ticker : "(disabled)")
    println("  overlay_out_png = ", out_png)
    assumptions_applied = Dict(
        "input_csv" => input_csv,
        "price_primary" => string(price_primary),
        "spread_bps" => spread_bps,
        "low_liq_q" => low_liq_q,
        "window_lengths" => window_lengths,
        "benchmark" => (bench_enabled ? bench_ticker : "(disabled)"),
        "overlay_out_png" => out_png,
        "report_path" => out_report,
    )

    overlay_plot(t, p; spread_bps=spread_bps, outpath=out_png)
    summary = stability_summary(
        df, p, t;
        window_lengths=window_lengths,
        low_liq_q=low_liq_q,
        benchmark_ticker=bench_ticker,
        benchmark_enabled=bench_enabled
    )
    println("Stability / uncertainty summary:")
    for (k, v) in summary
        println("  ", k, " => ", v)
    end
    verdict = decision_verdict(summary)
    println(verdict[:sentence])
    println(verdict[:recommendation])
    write_report(out_report; assumptions_applied=assumptions_applied, summary=summary, verdict=verdict, plot_path=out_png)
    println("Saved report to $(out_report)")
    println("Saved overlay plot to $(out_png)")
end